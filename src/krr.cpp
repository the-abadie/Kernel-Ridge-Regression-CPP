#include "eigen-3.4.0/Eigen/Eigen"
#include <omp.h>
#include "../include/krr.hpp"

using namespace Eigen;

[[nodiscard]]
MatrixXd compute_kernel(const MatrixXd& A, const MatrixXd& B, const kernelType KERNEL, const double sigma){
    double gamma = 0.0;

    // l2 predictions computed though efficient matrix operations
    if (KERNEL == GAUSSIAN){
        gamma = -1. / (sigma * sigma);

        VectorXd A_norm;
        VectorXd B_norm;

        //Parallelized if matrix is large.
        if(A.rows() * B.rows() > 10000) {
            #pragma omp parallel sections
            {
                #pragma omp section
                A_norm = A.rowwise().squaredNorm();
                
                #pragma omp section
                B_norm = B.rowwise().squaredNorm();
            }

        } 

        //Default (sequential) behavior
        else {
            A_norm = A.rowwise().squaredNorm();   // (n x 1)
            B_norm = B.rowwise().squaredNorm();   // (m x 1)
        }

        MatrixXd A_BT(A.rows(), B.rows()); 
        A_BT.noalias() = A*B.transpose();         // (n x m)

        return ((A_norm.replicate(1, B.rows()) 
               + B_norm.transpose().replicate(A.rows(), 1)
               - A_BT * 2.0)
               .array()*gamma).exp();             // (n x m)
    }

    // l1 (Taxicab) Norm
    // Parallelized for-loop
    else if (KERNEL == LAPLACIAN){
        gamma = -1. / sigma;
        MatrixXd D = MatrixXd::Zero(A.rows(), B.rows());

        #pragma omp parallel for collapse(2)
        for (int i = 0; i < A.rows(); ++i) {
            for (int j = 0; j < B.rows(); ++j) {
                D(i, j) = (A.row(i) - B.row(j)).lpNorm<1>();
            }
        }

        return (D.array() * gamma).exp();
    }

    else{
        std::cerr << "Invalid kernel type given in `KRR::compute_kernel`." << std::endl;
        exit(-1);
    }
}

//Sets alphas for `KRR` class based on kernel generated by the training data and targets.
//Assumes class has already been initialized with `sigma`, `lambda`, and `kernelType`.
//Currently supports l1 (Taxicab) and l2 (Gaussian) `kernelType` norms.
void KRR::fit(const MatrixXd& TRAININGDATA, const VectorXd& TRAININGTARGET){
    trainingData = TRAININGDATA;

    MatrixXd K = compute_kernel(TRAININGDATA, TRAININGDATA, kernel, sigma);

    const int N = trainingData.rows();
    const MatrixXd lambdaI = lambda * MatrixXd::Identity(N, N);

    // Cholesky Decomposition to solve for alphas
    alphas = ((K + lambdaI)).selfadjointView<Upper>().ldlt().solve(TRAININGTARGET);
};

void KRR::fit(MatrixXd& KERNELMATRIX, const VectorXd& TRAININGTARGET){

    KERNELMATRIX.diagonal().array() += lambda; //Modify in place

    alphas = KERNELMATRIX.selfadjointView<Upper>().ldlt().solve(TRAININGTARGET);

    KERNELMATRIX.diagonal().array() -= lambda; //Modify in place for reuse

}

//Provides predictions on testing data based on trained `KRR` model.
//Currently supports l1 (Taxicab) and l2 (Gaussian) `kernelType`.
[[nodiscard]]
VectorXd KRR::predict(const MatrixXd& TESTINGDATA) const{
    const int N_train = trainingData.rows();
    const int N_test  = TESTINGDATA .rows();

    VectorXd predictions = VectorXd::Zero(N_test);

    const MatrixXd K = compute_kernel(TESTINGDATA, trainingData, kernel, sigma);
  
    predictions.noalias() = K * alphas;

    return predictions;
};

double
KRR::evaluate(const MatrixXd& TESTINGDATA, const VectorXd& TESTINGTARGET, const lossMetric LOSS) const {
    const int N_test  = TESTINGDATA .rows();

    VectorXd predictions = VectorXd::Zero(N_test);

    const MatrixXd K = compute_kernel(TESTINGDATA, trainingData, kernel, sigma);
  
    predictions.noalias() = K * alphas;

    // Compute loss
    double error = 0.0;
    const VectorXd residual = (predictions - TESTINGTARGET).eval();

    if (LOSS == MAE) {
        error = residual.cwiseAbs().mean();}
    
    else if (LOSS == RMSE) {
        error = std::sqrt(residual.squaredNorm() / N_test);
    }

    return error;
};

double
KRR::evaluate_kernel(const MatrixXd& KERNELMATRIX, const VectorXd& TESTINGTARGET, const lossMetric LOSS) const {
  
    VectorXd predictions = KERNELMATRIX * alphas;

    // Compute loss
    double error = 0.0;
    const VectorXd residual = (predictions - TESTINGTARGET);

    if (LOSS == MAE) {
        error = residual.cwiseAbs().mean();}
    
    else if (LOSS == RMSE) {
        error = std::sqrt(residual.squaredNorm() / TESTINGTARGET.rows());
    }
    else{
        std::cerr << "Invalid kernel type given in `KRR::evaluate`." << std::endl;
        exit(-1);
    }

    return error;
};